# Large-Scale Dataset and Benchmarking for Hand and Face Detection Focused on Sign Language

Detecting the hands and the face is an important task for sign language, once these channels have most part of the information used to classify the signs. This repository includes the source code, pretrained models, and the large-scale hand and face dataset for object detection. Although the models and dataset can be used for other problems, they are specially designed for sign language.

## Sign Language Hand and Face Dataset
The large-scale hand and face dataset for sign language is based on the [AUTSL](https://chalearnlap.cvc.uab.cat/dataset/40/description/) dataset, which contains 43 interpreters, 20 backgrounds, and more than 36,000 videos. To create the annotations, we trained an initial detector using the [Autonomy](https://autonomy.cs.sfu.ca/hands_and_faces/) data. After that, we employed this initial model and an [auto-annotation tool](https://github.com/AlvaroCavalcante/auto_annotate) to generate the annotations following the PASCAL VOC format. Finally, we manually reviwed all the images and the bounding boxes to fix the mistakes made by the model and better fit the objects. The generated dataset has the following statistics:

| Frames  | Hands  | Faces  |
|---|---|---|
| 477,480  |  954,960 | 477,480

**NOTE:** We tried to detect a maximum of 16 frames per video using a confidence threshold of 35%, but in some cases the model was not able to detect the desired objects. The Figure below shows some samples of the dataset.

![Image](/assets/hand_face_example.png "Annotated dataset")
### Dataset split
The dataset was split following the [Chalearn](https://chalearnlap.cvc.uab.cat/dataset/40/description/) competition guidelines. That said, we used 31 interpreters for training, 6 for validation, and 6 for testing, ensuring that the same interpreter did not appear in multiple splits. The number of images per split was 369,053 for training, 49,041 for test, and 59,386 for validation.


### TFRecord creation
To train the models using TensorFlow, we first need to convert the images and XML annotations into TFRecord format. To do so, we first need to create a csv file that maps the images with the respective annotations using the command bellow:

```
python3 utils/xml_to_csv.py -i /xml-input-path -o /csv-output-path
```
The "xml-input-path" is the path to the folder containing the XML files with the annotations, and the "csv-output-path" is the path to the output CSV file. After that, we can create the TFRecord files by running the following command:

```
python3 utils/generate_tfrecord.py --csv_input=/path-to-csv --output_path ./output.record --img_path=/path-to-images --label_map=/path-to-label_map.pbtxt --n_splits n_files_to_generate 
```
The command parameters are the following:
- csv_input: The path of the csv file generated by the previous script.
- output_path: The path to the output TFRecord files.
- img_path: The path to the folder containing the images.
- label_map: The path to the label_map.pbtxt file.
- n_splits: The number of TFRecord files to generate.

In this case, we recommend the creation of 15 TFRecord files for test and validation sets, and 110 files for training. Every file has around 200 MB.

### Yolo annotations
To train the Yolo model, it's necessary to convert the XML annotations to TXT format. We can do this by running the following command:
```
python3 utils/xml2yolo.py
```
**NOTE:** Before running the script, make sure to change the labels, images and output paths in the beginning of the script.
**NOTE 2:** We already provide the Yolo annotations in the dataset folder, so you don't need to run the script.

After that, you can visualize the yolo bounding boxes in the images by running the following command:
```
python3 utils/show_yolo_img.py
```
> Both scripts were obtained from [this](https://towardsdatascience.com/convert-pascal-voc-xml-to-yolo-for-object-detection-f969811ccba5) tutorial.

## Object Detection Results
We trained and optimized different object detection architectures for the given task of hand and face detection for sign language, achieving good results while reducing the models' complexity. The Table bellow shows the mean Average Precision (mAP) and inference time (milliseconds) of each detector, where the values in parentheses correspond to the inference time before applying any optimizations.

**Note:** CPU Intel Core I5 10400, GPU Nvidia RTX 3060.

| Architecture  | Inf. time CPU  | Inf. time GPU  | mAP@50 | mAP@75 |
|---|---|---|---|---|
| SSD640  |  53.2 (108.0) | 11.6 (44.1) | 98.5 | 95.0
| SSD320  |  **15.7** (32.7) | 9.9 (25.7) | 92.1 | 73.1
| EfficientDet D0  |  67.8 (124.5) | 16.1 (53.4) | 96.7 | 85.8
| YoloV7  |  123.9 (211.1) | **7.4** (7.6) | 98.6 | 95.7
| Faster R-CNN  |  281.0 (811.5) | 26.3 (79.1) | **99.0** | 96.2
| CenterNet  |  40.0 | 7.9 | **99.0** | **96.7**

The models were trained using the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/official#object-detection-and-segmentation) and the configuration file of each architecture can be found at **src/utils/pipelines**, making it easy to reproduce the results. 

## **Training the Model**
To train the object detector, the first step is to execute the model setup, by running the following script:

```
python3 hand_face_detector_setup.py --label_map_path /label_map.pbtxt --batch_size 50 --model_name "Your model name" --download_url http://tf-url.com
```

Where the model name and download url is taken from [TF Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). The setup script basically configures the pipeline file to prepare the model to be trained.

After the model setup, you just need to run the following code to start the detector training:
```
python models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path={pipeline_fname} \
    --model_dir={model_dir} \
    --alsologtostderr \
    --num_train_steps={num_steps} \
    --checkpoint_every_n=1000 \
    --num_eval_steps={num_eval_steps}
```
The model training was conducted on Google Colab. The notebook can be found [here](https://colab.research.google.com/drive/1209hYjuj449H-H_jfXLMdvnSgHYWgsq0?usp=sharing).

## **Evaluating model performance**
To evaluate the model performance during the training step, you just need to run the following command:

```
python models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path={pipeline_fname} \
    --model_dir={model_dir} \
    --alsologtostderr \
    --eval_on_train_data=True \
    --checkpoint_dir={model_dir} \
```

When specifying the **checkpoint_dir** parameter, the last checkpoint will be used to evaluate the model performance on eval data.

## Exporting trained model
After training a great model, you probably would like to export it into the savedModel format to use it in the inference code. To do so, just run the following command:

```
python models/research/object_detection/exporter_main_v2.py \
    --input_type image_tensor \
    --pipeline_config_path {pipeline_fname} \
    --trained_checkpoint_dir {model_dir} \
    --output_directory {output_path}
```

## Testing Model Results
To verify the model working, we need to run the **hand_face_detection.py** script, with the following arguments:

- **saved_model_path**: Path of the saved_model
- **source_path**: Path of the video file to test the model. The default behavior is to use the webcam stream.
- **label_map_path**: Path of the label map used by the model.
- **compute_features**: If True, the trigronometrical features are calculated. 
- **use_docker**: Removes result visualization when running inside docker container.
- **single_person**: Define a limit of hands and face to detect considering one single person.

The **asl_bench.mp4** video, inside utils/test_videos folder is used as a commom benchmarking for the trained models, where the goal is to verify the mean FPS.

## Running Project on Nvidia Docker
An easy way to use your GPU is through the Nvidia Docker images. To do so, you can follow [this](https://www.tensorflow.org/install/docker?hl=pt-br) simple documentation. The first step is to install the nvidia support to docker, which can be done following [this](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker) tutorial.

You'll need to setup the repo into the GPG key:
```
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \
   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
```
After that, just run:
```
sudo apt update
sudo apt-get install -y nvidia-container-toolkit
```
You can test if it worked by using this command:
```
docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \
   python -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
```
Finally, you'll be able to run this project using your docker. To do so, first, build a new container using the Dockerfile in the project root:
```
sudo docker build . -t tf-gpu --network=host
```
**NOTE:** use --network to able internet connection.

To finish, you just need to run the following command:
```
sudo docker run --gpus all -it --rm -v $PWD:/tmp -w /tmp tf-gpu python hand_face_detection.py --source_path ./utils/test_videos/asl_bench.mp4
```

This command need to be executed inside this repository, once you'll copy all the files in your working directory into the docker container, and run itself overthere! 